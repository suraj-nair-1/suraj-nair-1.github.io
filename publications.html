<!-- 1. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://www.physicalintelligence.company/blog/pi0"><papertitle>π₀: A Vision-Language-Action Flow Model for General Robot Control</papertitle></a><br/>
    Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, <strong>Suraj Nair</strong>, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky<br/>
    <p>
      A foundation model for robotics that combines a pre-trained vision-language model with flow matching to enable general robot control across diverse platforms and tasks.
    </p>
  </div>
</div>

<!-- 2. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://openvla.github.io/"><papertitle>OpenVLA: An Open-Source Vision-Language-Action Model</papertitle></a><br/>
    Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, <strong>Suraj Nair</strong>, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn<br/>
    <em>Conference on Robot Learning (CoRL), 2024</em><br/>
    <strong>Outstanding Paper Award Finalist</strong>
    <p>
      An open-source 7B-parameter vision-language-action model trained on diverse robot demonstrations that outperforms larger closed models and can be efficiently fine-tuned on consumer GPUs for new robotic tasks.
    </p>
  </div>
</div>

<!-- 3. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://droid-dataset.github.io/"><papertitle>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</papertitle></a><br/>
    Alexander Khazatsky, Karl Pertsch, <strong>Suraj Nair</strong>, Ashwin Balakrishna, ..., Thomas Kollar, Sergey Levine, Chelsea Finn<br/>
    <em>Robotics: Science and Systems (RSS), 2024</em>
    <p>
      A diverse robot manipulation dataset with 76k demonstrations across 564 scenes and 84 tasks, collected by 50 data collectors worldwide, showing improved performance and generalization for policies trained on this data.
    </p>
  </div>
</div>

<!-- 4. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://github.com/TRI-ML/prismatic-vlms"><papertitle>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</papertitle></a><br/>
    Siddharth Karamcheti, <strong>Suraj Nair</strong>, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh<br/>
    <em>International Conference on Machine Learning (ICML), 2024</em><br/>
    <p>
      Systematic investigation of key design decisions in visually-conditioned language models, offering standardized evaluations, optimized training code, and high-performing VLMs that outperform state-of-the-art open models.
    </p>
  </div>
</div>

<!-- 5. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/view/voltron-robotics"><papertitle>Language-Driven Representation Learning for Robotics</papertitle></a><br/>
    Siddharth Karamcheti, <strong>Suraj Nair</strong>, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang<br/>
    <em>Robotics: Science and Systems (RSS), 2023</em><br/>
    <strong>Best Paper Award Finalist</strong><br/>
    <p>
      Voltron, a multi-modal foundation model for robotics trained on human videos and language that produces reusable representations and rewards with capabilities spanning control features, expression grounding, and intent inference.
    </p>
  </div>
</div>

<!-- 6. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/view/behaviorretrieval"><papertitle>Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets</papertitle></a><br/>
    Maximilian Du, <strong>Suraj Nair</strong>, Dorsa Sadigh, Chelsea Finn<br/>
    <em>Robotics: Science and Systems (RSS), 2023</em><br/>
    <p>
      A novel approach for leveraging past project data to improve downstream learning by using a learned model to select relevant data from large datasets of robot interactions, enhancing small task demonstration sets for more efficient behavior cloning.
    </p>
  </div>
</div>

<!-- 7. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/view/robot-r3m/"><papertitle>R3M: A Universal Visual Representation for Robot Manipulation</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta<br/>
    <em>Conference on Robot Learning (CoRL), 2022</em><br/>
    <em>ICRA Scaling Robot Learning Workshop (Best Paper Award)</em><br/>
    <p>
      A generalizable visual representation pre-trained on diverse human videos and language that enables significantly more efficient learning across a wide range of robotic manipulation tasks.
    </p>
  </div>
</div>

<!-- 8. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/view/playitbyear/"><papertitle>Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual Imitation Learning</papertitle></a><br/>
    Maximilian Du*, Olivia Y. Lee*, <strong>Suraj Nair</strong>, and Chelsea Finn<br/>
    <em>Robotics: Science and Systems (RSS), 2022</em><br/>
    <p>
      A method enabling robots to tackle challenging visually occluded manipulation tasks (like extracting keys from a bag) through end-to-end interactive imitation learning from vision and sound.
    </p>
  </div>
</div>

<!-- 9. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/view/robotlorel"><papertitle>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, Chelsea Finn<br/>
    <em>Conference on Robot Learning (CoRL), 2021</em><br/>
    <p>
      An approach for acquiring language-conditioned visuomotor skills on real robots using entirely offline, pre-collected datasets and crowdsourced language annotation.
    </p>
  </div>
</div>

<!-- 10. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/view/embr-site"><papertitle>Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks</papertitle></a><br/>
    Bohan Wu, <strong>Suraj Nair</strong>, Li Fei-Fei*, Chelsea Finn*<br/>
    <em>Conference on Robot Learning (CoRL), 2021</em><br/>
    <p>
      EMBR, a model-based RL algorithm that learns visuomotor skills and their groundings, which can then be sequenced with symbolic planners to complete long-horizon, multi-stage manipulation tasks on real robots.
    </p>
  </div>
</div>

<!-- 11. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://arxiv.org/abs/2106.13195"><papertitle>FitVid: Overfitting in Pixel-Level Video Prediction</papertitle></a><br/>
    Mohammad Babaeizadeh, Mohammad Taghi Saffar, <strong>Suraj Nair</strong>, Sergey Levine, Chelsea Finn, Dumitru Erhan<br/>
    <em>Arxiv Preprint, 2021</em><br/>
    <p>
      A variational video prediction model capable of severe overfitting on standard benchmarks while maintaining a parameter count similar to current state-of-the-art models.
    </p>
  </div>
</div>

<!-- 12. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/view/dvd-human-videos"><papertitle>Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos</papertitle></a><br/>
    Annie S. Chen, <strong>Suraj Nair</strong>, Chelsea Finn<br/>
    <em>Robotics Science and Systems (RSS), 2021</em><br/>
    <em>ICLR Workshop on Self-Supervised Reinforcement Learning (Oral), 2021</em><br/>
    <p>
      A technique for learning multi-task reward functions from limited robot data and extensive in-the-wild human videos, enabling generalization to new environments and tasks through diverse human data.
    </p>
  </div>
</div>

<!-- 13. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/view/ghvae"><papertitle>Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</papertitle></a><br/>
    Bohan Wu, <strong>Suraj Nair</strong>, Roberto Martin-Martin, Li Fei-Fei*, Chelsea Finn*<br/>
    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</em><br/>
    <p>
      A technique for video prediction that trains a hierarchy of action-conditioned VAEs in a greedy fashion, allowing efficient training of large video prediction models.
    </p>
  </div>
</div>

<!-- 14. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/berkeley.edu/mbold"><papertitle>Model-Based Visual Planning with Self-Supervised Functional Distances</papertitle></a><br/>
    Stephen Tian, <strong>Suraj Nair</strong>, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, Sergey Levine<br/>
    <em>International Conference on Learning Representations (ICLR), 2021 (Spotlight)</em><br/>
    <p>
      An offline model-based RL method combining a video prediction model with a Q function-based distance metric to accomplish visually specified goals.
    </p>
  </div>
</div>

<!-- 15. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/view/batch-exploration"><papertitle>Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</papertitle></a><br/>
    Annie S. Chen*, HyunJi Nam*, <strong>Suraj Nair*</strong>, Chelsea Finn<br/>
    <em>RA-L / ICRA, 2021</em><br/>
    <p>
      A framework leveraging minimal human supervision to enhance robotic exploration, enabling high-quality unsupervised data collection that improves downstream offline RL performance.
    </p>
  </div>
</div>

<!-- 16. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/berkeley.edu/recovery-rl/"><papertitle>Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones</papertitle></a><br/>
    Brijen Thananjeyan*, Ashwin Balakrishna*, <strong>Suraj Nair</strong>, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E. Gonzalez, Julian Ibarz, Chelsea Finn, Ken Goldberg<br/>
    <em>RA-L / ICRA, 2021</em><br/>
    <p>
      An algorithm for safe reinforcement learning utilizing offline data to learn constraints before policy learning, with dual policies separating exploration and constraint satisfaction objectives.
    </p>
  </div>
</div>

<!-- 17. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/stanford.edu/gap"><papertitle>Goal-Aware Prediction: Learning to Model what Matters</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Silvio Savarese, Chelsea Finn<br/>
    <em>International Conference on Machine Learning (ICML), 2020</em><br/>
    <p>
      An exploration of visual dynamics models conditioned on goals, focusing the learning process exclusively on goal-relevant quantities.
    </p>
  </div>
</div>

<!-- 18. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://sites.google.com/stanford.edu/hvf"><papertitle>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Chelsea Finn<br/>
    <em>International Conference on Learning Representations (ICLR), 2020</em><br/>
    <p>
      A study of long-horizon vision-based task learning in self-supervised settings through hierarchical visual foresight, which optimizes for subgoal sequences that decompose tasks into simpler sub-steps.
    </p>
  </div>
</div>

<!-- 19. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/view/time-reversal"><papertitle>Time Reversal as Self-Supervision</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Mohammad Babaeizadeh, Chelsea Finn, Sergey Levine, Vikash Kumar<br/>
    <em>International Conference on Robotics and Automation (ICRA), 2020</em><br/>
    <p>
      A technique utilizing time-reversal to discover goals and generate high-level plans by exploring outward from goal states, "unsolving" tasks, then reversing the process during testing.
    </p>
  </div>
</div>

<!-- 20. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://sites.google.com/stanford.edu/visualcausalinduction"><papertitle>Causal Induction from Visual Observations for Goal-Directed Tasks</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Yuke Zhu, Silvio Savarese, Li Fei-Fei<br/>
    <em>Workshop on Causal ML at NeurIPS, 2019</em><br/>
    <p>
      An exploration of effective causal graph prediction from limited visual observations, with integration of learned graphs into downstream goal-conditioned policy learning.
    </p>
  </div>
</div>

<!-- 21. Publication - Highlighted -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://www.robonet.wiki/"><papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle></a><br/>
    Sudeep Dasari, Frederik Ebert, Stephen Tian, <strong>Suraj Nair</strong>, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, Chelsea Finn<br/>
    <em>Conference on Robot Learning (CoRL), 2019</em><br/>
    <p>
      A dataset of robotic experience gathered across 4 institutions and 7 robots, demonstrating how algorithms leveraging this data can adapt to new environments more rapidly than training from scratch.
    </p>
  </div>
</div>

<!-- 22. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://arxiv.org/abs/1807.03480"><papertitle>Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration</papertitle></a><br/>
    De-An Huang*, <strong>Suraj Nair*</strong>, Danfei Xu*, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles<br/>
    <em>IEEE CVPR, 2019 (Oral)</em><br/>
    <p>
      NTG generates task graphs from single video demonstrations of unseen tasks, enabling one-shot imitation learning.
    </p>
  </div>
</div>

<!-- 23. Publication -->
<div class="publication-card highlighted">
  <div class="publication-content">
    <a href="https://stanfordvl.github.io/ntp/"><papertitle>Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</papertitle></a><br/>
    Danfei Xu*, <strong>Suraj Nair*</strong>, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, Silvio Savarese<br/>
    <em>ICRA, 2018</em><br/>
    <p>
      A meta-learning framework that generates robot-executable neural programs from task demonstration videos.
    </p>
  </div>
</div>

<!-- 24. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://arxiv.org/abs/1901.03467"><papertitle>Reliable RealTime Seismic Signal/Noise Discrimination With Machine Learning</papertitle></a><br/>
    Men-Andrin Meier, Zachary E Ross, Anshul Ramachandran, Ashwin Balakrishna, <strong>Suraj Nair</strong>, Peter Kundzicz, Zefeng Li, Jennifer Andrews, Egill Hauksson, Yisong Yue<br/>
    <em>Journal of Geophysical Research: Solid Earth, 2019</em>
    <p>
      An efficient system for distinguishing real local earthquake signals from impulsive noise to improve earthquake early warning (EEW) alerts.
    </p>
  </div>
</div>

<!-- 25. Publication -->
<div class="publication-card">
  <div class="publication-content">
    <a href="https://ieeexplore.ieee.org/document/8284202"><papertitle>Annotated Reconstruction of 3D Spaces Using Drones</papertitle></a><br/>
    <strong>Suraj Nair</strong>, Anshul Ramachandran, Peter Kundzicz<br/>
    <em>MIT Undergraduate Research in Technology Conference (URTC), 2017 (Best Paper Presentation)</em>
    <p>
      A system for 3D voxel scene reconstruction with object labels from RGB drone images, applied to exploratory motion planning.
    </p>
  </div>
</div>