<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suraj Nair</title>
  
  <meta name="author" content="Suraj Nair">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/tri.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Suraj Nair</name>
              </p>
              <p>I am a research scientist on the <a href="https://www.tri.global/our-work/machine-learning">ML Research Team</a> at the Toyota Research Institute, where I work at the intersection of machine learning, robotics, and computer vision. <em>My research focuses on leveraging language, video, and robot data to train foundation models for embodied AI.</em> 
                
              <p>
                I completed my Ph.D. in Computer Science from the Stanford AI Lab, where I was co-advised by Professors <a href="https://ai.stanford.edu/~cbfinn/"> Chelsea Finn</a> and  <a href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a>. My Ph.D. thesis was on <a href="https://stacks.stanford.edu/file/druid:fk655fk4359/suraj_nair_thesis_final-augmented.pdf"> Scaling Deep Robotic Learning to Broad Real-World Data</a>. Before that, I completed my Bachelors in Computer Science at the California Institute of Technology (Caltech), where I worked with <a href="http://www.yisongyue.com">Yisong Yue</a> on multi-agent reinforcement learning. I have also spent time at <a href="https://ai.facebook.com/"> Facebook AI Research</a>, <a href="https://ai.google/research/teams/brain">Google Brain</a>, and <a href="https://www.ge.com/digital/">GE</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:surajnair0220@gmail.edu">Email</a> &nbsp/&nbsp
                <a href="data/cv082023.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Suraj_Research_Statement_May22.pdf">Research Statement (May 2022)</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=EHSuFcwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/SurajNair_1">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/surajnaircaltech/">LinkedIn</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:35%;max-width:35%">
              <a href="images/headshot_2021_3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_2021_3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Research</heading>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              
<!--               <p> The goal of my research is to enable robotic agents which can generalize effectively across tasks, objects, and environments by learning from large datasets. To that end my research focuses on methods for scalably collecting robotic data as well as offline reinforcement learning algorithms which can learn to solve many tasks with limited supervision. 
              <p> -->
                <p> The goal of my research is to enable robots that can operate in unstructured, real-world environments. Towards this goal I study how robots can generalize effectively across tasks, objects, and environments by learning from large datasets. Specifically, my research focuses on methods for:
                  <ul>
                    <li>Scalably [<a href="https://arxiv.org/abs/1910.11215">1</a>, <a href="https://arxiv.org/abs/2010.11917">2</a>] and safely [<a href="https://arxiv.org/abs/2010.15920">3</a>] collecting real world robot datasets.</li>
                    <li>Self-supervised learning of visual models from offline data [<a href="https://arxiv.org/abs/2007.07170">1</a>,<a href="https://arxiv.org/abs/2103.04174">2</a>,<a href="https://arxiv.org/abs/2106.13195">3</a>] and using them for robotic manipulation [<a href="https://arxiv.org/abs/1810.01128">4</a>,<a href="https://arxiv.org/abs/1909.05829">5</a>,<a href="https://arxiv.org/abs/2012.15373">6</a>,<a href="https://arxiv.org/abs/2109.10312">7</a>].</li>
                    <li>Leveraging human video datasets [<a href="https://arxiv.org/abs/2103.16817">1</a>,<a href="https://arxiv.org/abs/2203.12601">3</a>] and natural language annotations [<a href="https://arxiv.org/abs/2109.01115">2</a>,<a href="https://arxiv.org/abs/2203.12601">3</a>] to enable better robot learning.</li>
                  </ul>
              <p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Recent Talks (April 2022 @ Nuro)</heading>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <iframe src="https://drive.google.com/file/d/1lRSVA6BrOfC3Qrh82d3VhzYFmalMFE4b/preview" width="640" height="480" allow="autoplay"></iframe>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications & Preprints (<span style="background-color:#F3EEB5">Highlighted Papers</span>) </heading>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/voltron.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2302.12766" id="MCG_journal">
                <papertitle>Language-Driven Representation Learning for Robotics</papertitle>
              </a>
              <br>
              Siddharth Karamcheti, <strong>Suraj Nair</strong>, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2023
	          <br>
              <strong>Best Paper Award Finalist</strong>
	          <br>
              <a href="https://sites.google.com/view/voltron-robotics">project page</a> /
              <a href="https://github.com/siddk/voltron-robotics">code</a> 
              <p></p>
              <p> We present Voltron, a multi-modal foundation model for robotics trained on human videos and language to produce reusable representations and rewards. We train a single model with many downstream capabilities from features for control to expression grounding and reward/intent inference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/behaviorret.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2304.08742" id="MCG_journal">
                <papertitle>Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets.</papertitle>
              </a>
              <br>
              Maximilian Du, <strong>Suraj Nair</strong>, Dorsa Sadigh, Chelsea Finn
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2023
	          <br>
              <a href="https://sites.google.com/view/behaviorretrieval">project page</a> /
              <a href="https://github.com/MaxDu17/BehaviorRetrieval">code</a> / 
              <p></p>
              <p>Often, robot data isn't shared across projects. We present a new way that past project data can be used to improve downstream learning. We use a learned model select relevant data from a large dataset of robot interactions, which augments a small set of task demonstrations for use in a behavior cloning algorithm for more efficient learning. </p>
            </td>
          </tr>

          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/r3m.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.12601" id="MCG_journal">
                <papertitle>R3M: A Universal Visual Representation for Robot Manipulation</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta
              <br>
              <em>Conference on Robot Learning (CoRL) </em> 2022 
              <br>
              <em>ICRA Scaling Robot Learning Workshop </em> 2022, <strong>(Best Paper Award)</strong>
	          <br>
              <a href="https://sites.google.com/view/robot-r3m/">project page</a> /
              <a href="https://github.com/facebookresearch/r3m">code</a>
              <p></p>
              <p>We pre-train a generalizable visual representation on diverse human videos and language, and show it enables far more efficient learning across a wide range of robotic manipulation tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ear.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2205.14850" id="MCG_journal">
                <papertitle>Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual Imitation Learning</papertitle>
              </a>
              <br>
              Maximilian Du*, Olivia Y. Lee*, <strong>Suraj Nair</strong>, and Chelsea Finn
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2022
	          <br>
              <a href="https://sites.google.com/view/playitbyear/">project page</a> /
              <a href="https://github.com/MaxDu17/PlayItByEar_Code">code</a> / 
              <a href="https://www.newscientist.com/article/2323894-robot-can-find-keys-in-a-bag-just-by-listening-as-it-rummages-around/">press</a>
              <p></p>
              <p>We propose a method to enable robots to tackle challenging visually occluded manipulation tasks (like extracting keys from a bag), via end-to-end  interactive imitation learning from vision and sound. .</p>
            </td>
          </tr>
          

          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lorl.jpeg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2109.01115" id="MCG_journal">
                <papertitle>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, Chelsea Finn
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021
              <br>
              <a href="https://sites.google.com/view/robotlorel">project page</a> /
              <a href="https://github.com/suraj-nair-1/lorel">code</a>
              <p></p>
              <p>We learn language-conditioned visuomotor skills on real robots from entirely offline, pre-collected datasets and crowdsourced language annotation.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/embr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2109.10312" id="MCG_journal">
                <papertitle>Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks</papertitle>
              </a>
              <br>
              Bohan Wu, <strong>Suraj Nair</strong>, Li Fei-Fei*, Chelsea Finn*
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021
              <br>
              <a href="https://sites.google.com/view/embr-site">project page</a>
              <p></p>
              <p>EMBR is a model-based RL algorithm that learns visuomotor skills and their groundings, which can then be sequenced with symbolic planners to complete long-horizon, multi-stage manipulation tasks on real robots.   </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fitvid.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.13195" id="MCG_journal">
                <papertitle>FitVid: Overfitting in Pixel-Level Video Prediction</papertitle>
              </a>
              <br>
              Mohammad Babaeizadeh, Mohammad Taghi Saffar, <strong>Suraj Nair</strong>, Sergey Levine, Chelsea Finn, Dumitru Erhan
              <br>
              <em>Arxiv Preprint</em>, 2021
	          <br>
              <a href="https://sites.google.com/view/fitvidpaper/">project page</a> /
              <a href="https://github.com/google-research/fitvid">code</a>
              <p></p>
              <p>We propose a variational video prediction model that is capable of severe overfitting on common video prediction benchmarks while having similar parameter count as the current SOTA models.</p>
            </td>
          </tr>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dvd.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.16817" id="MCG_journal">
                <papertitle>Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos</papertitle>
              </a>
              <br>
              Annie S. Chen, <strong>Suraj Nair</strong>, Chelsea Finn
              <br>
              <em>Robotics Science and Systems (RSS)</em>, 2021
	      <br>
              <em>ICLR Workshop on Self-Supervised Reinforcement Learning</em>, 2021, <strong>(Oral)</strong>
              <br>
              <a href="https://sites.google.com/view/dvd-human-videos">project page</a> 
<!--               <a href="https://github.com/suraj-nair-1/goal_aware_prediction">code</a> -->
              <p></p>
              <p>We propose a technique for learning multi-task reward functions from a small amount of robot data and large amounts of in-the-wild human videos. By leveraging diverse human data, the learned reward function is able to generalize to new environments and tasks.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ghvae.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.04174" id="MCG_journal">
                <papertitle>Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</papertitle>
              </a>
              <br>
              Bohan Wu, <strong>Suraj Nair</strong>, Roberto Martin-Martin, Li Fei-Fei*, Chelsea Finn*
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2021
              <br>
              <a href="https://sites.google.com/view/ghvae">project page</a> 
<!--               <a href="https://github.com/suraj-nair-1/goal_aware_prediction">code</a> -->
              <p></p>
              <p>We propose a technique for video prediction which trains a hierarchy of action-conditioned VAEs in a greedy fashion, enabling efficient training of large video prediction models.</p>
            </td>
          </tr>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mbold.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2012.15373" id="MCG_journal">
                <papertitle>Model-Based Visual Planning with Self-Supervised Functional Distances</papertitle>
              </a>
              <br>
              Stephen Tian, <strong>Suraj Nair</strong>, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, Sergey Levine
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2021 <strong>(Spotlight)</strong>
              <br>
              <a href="https://sites.google.com/berkeley.edu/mbold">project page</a> 
<!--               <a href="https://github.com/suraj-nair-1/goal_aware_prediction">code</a> -->
              <p></p>
              <p>We propose a method for offline model-based RL which learns a video prediction model and a Q function based distance metric, and uses them to accomplish visually specified goals.</p>
            </td>
          </tr>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_bee_4.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.11917" id="MCG_journal">
                <papertitle>Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</papertitle>
              </a>
              <br>
              Annie S. Chen*, HyunJi Nam*, <strong>Suraj Nair*</strong>, Chelsea Finn
              <br>
              <em>Robotics and Automation Letters (RA-L) and International Conference on Robotics and Automation (ICRA)</em>, 2021
              <br>
              <a href="https://sites.google.com/view/batch-exploration">project page</a> /
              <a href="https://github.com/stanford-iris-lab/batch-exploration">code</a>
              <p></p>
              <p>We propose a framework for leveraging weak human superivision to enable better robotic exploration. Using just a few minutes of human supervision, the robot collects high quality data while unsupervised, providing better data for downstream offline RL. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rrl_img.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.15920" id="MCG_journal">
                <papertitle>Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones</papertitle>
              </a>
              <br>
              Brijen Thananjeyan*, Ashwin Balakrishna*, <strong>Suraj Nair</strong>, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E. Gonzalez, Julian Ibarz, Chelsea Finn, Ken Goldberg
              <br>
              <em>Robotics and Automation Letters (RA-L) and International Conference on Robotics and Automation (ICRA)</em>, 2021
              <br>
              <a href="https://sites.google.com/berkeley.edu/recovery-rl/">project page</a> 
<!--               <a href="https://github.com/suraj-nair-1/goal_aware_prediction">code</a> -->
              <p></p>
              <p>An algorithm for safe reinforcement learning which utilizes a set of offline data to learn about constraints before policy learning and a pair of policies which seperate the often conflicting objectives of task directed exploration and constraint satisfaction to learn contact rich and visuomotor control tasks. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gap.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.07170" id="MCG_journal">
                <papertitle>Goal-Aware Prediction: Learning to Model what Matters</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Silvio Savarese, Chelsea Finn
              <br>
              <em>International Conference on Machine Learning (ICML) </em>, 2020
              <br>
              <a href="https://sites.google.com/stanford.edu/gap">project page</a> /
              <a href="https://github.com/suraj-nair-1/goal_aware_prediction">code</a>
              <p></p>
              <p>We explore learning visual dynamics models which are conditioned on goals, and learn to model only goal relevant quantities.</p>
            </td>
          </tr>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hvf.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1909.05829" id="MCG_journal">
                <papertitle>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation.</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Chelsea Finn
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020
              <br>
              <a href="https://sites.google.com/stanford.edu/hvf">project page</a> /
              <a href="https://github.com/suraj-nair-1/google-research/tree/master/hierarchical_foresight">code</a>
              <p></p>
              <p>We study how we can learn long horizon vision-based tasks in self-supervised settings. Our approach, hierarchical visual foresight, can optimize for a sequence of subgoals that break down the task into easy to complete subsegments.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/trass_2.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1810.01128" id="MCG_journal">
                <papertitle>Time Reversal as Self-Supervision</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Mohammad Babaeizadeh, Chelsea Finn, Sergey Levine, Vikash Kumar
              <br>
              <em>International Conference on Robotics and Automation (ICRA) </em>, 2020
              <br>
              <a href="https://sites.google.com/view/time-reversal">project page</a> /
              <a href="https://venturebeat.com/2020/05/26/google-ai-researchers-want-to-teach-robots-tasks-through-self-supervised-reverse-engineering/">press</a>
              <p></p>
              <p>We propose a technique that uses time-reversal to learn goals and provide a high level plan to reach them. In particular, our approach explores outward from a set of goal states, "unsolving" a task, which then enables solving the task from new initializations at test time.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icin.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1910.01751" id="MCG_journal">
                <papertitle>Causal Induction from Visual Observations for Goal-Directed Tasks</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Yuke Zhu, Silvio Savarese, Li Fei-Fei
              <br>
              <em>Workshop on Causal Machine Learning NeurIPS</em>, 2019
              <br>
              <a href="https://sites.google.com/stanford.edu/visualcausalinduction">project page</a> /
              <a href="https://github.com/StanfordVL/causal_induction">code</a>
              <p></p>
              <p>We explore how to effectively predict causal graphs from a small set of visual observations, and how to encorporate the learned graphs into downstream goal conditioned policy learning.</p>
            </td>
          </tr>
          
          <tr bgcolor= "#F3EEB5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/robonet.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1910.11215" id="MCG_journal">
                <papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle>
              </a>
              <br>
              Sudeep Dasari, Frederik Ebert, Stephen Tian, <strong>Suraj Nair</strong>, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, Chelsea Finn
              <br>
               <em> Conference on Robot Learning (CoRL) </em>, 2019
              <br>
              <a href="https://www.robonet.wiki/">project page</a> /
              <a href="https://github.com/SudeepDasari/RoboNet">code</a> /
              <a href="https://www.technologyreview.com/s/614668/welcome-to-robot-university-only-robots-need-apply/">press</a>
              <p></p>
              <p>We collect a dataset of robotic experience across 4 institutions and 7 robots, and demonstrate that robot learning algorithms leveraging this data can adapt to new environments faster than training from scratch.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ntg.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1807.03480" id="MCG_journal">
                <papertitle> Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration</papertitle>
              </a>
              <br>
              De-An Huang*, <strong>Suraj Nair*</strong>, Danfei Xu*, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2019 <strong>(Oral)</strong>
              <br>
              <p></p>
              <p>NTG learns to produce a task graph from a single video demonstration of an unseen task, and leverages it for one-shot imitation learning.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ntp.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1710.01813" id="MCG_journal">
                <papertitle> Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</papertitle>
              </a>
              <br>
              Danfei Xu*, <strong>Suraj Nair*</strong>, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, Silvio Savarese
              <br>
               <em>International Conference on Robotics and Automation (ICRA) </em>, 2018
              <br>
              <a href="https://stanfordvl.github.io/ntp/">project page</a> /
              <a href="https://github.com/StanfordVL/ntp">code</a> / 
              <a href="https://www.youtube.com/watch?v=Lcxz6dtYjI4">Two Minute Papers</a> 
              <p></p>
              <p>Neural Task Programming (NTP) is a meta-learning framework that learns to generate robot-executable neural programs from task demonstration video.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eew.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1901.03467" id="MCG_journal">
                <papertitle> Reliable RealTime Seismic Signal/Noise Discrimination With Machine Learning</papertitle>
              </a>
              <br>
              Men-Andrin Meier, Zachary E Ross, Anshul Ramachandran, Ashwin Balakrishna, <strong>Suraj Nair</strong>, Peter Kundzicz, Zefeng Li, Jennifer Andrews, Egill Hauksson, Yisong Yue.
              <br>
              <em>Journal of Geo-Physical Research: Solid Earth</em>, 2019
              <br>
              <p></p>
              <p>Efficient prediction of real local earthquake signals from impulsive signals for earthquake early warning (EEW) alerts.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rec.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/8284202" id="MCG_journal">
                <papertitle> Annotated Reconstruction of 3D Spaces Using Drones</papertitle>
              </a>
              <br>
              <strong>Suraj Nair</strong>, Anshul Ramachandran, Peter Kundzicz.
              <br>
              <em>MIT Undergraduate Research in Technology Conference (URTC)</em>, 2017 <strong>(Best Paper Presentation)</strong>
              <br>
              <p></p>
              <p>Reconstruct 3D voxel representations of a scene with object labels from RGB images captured from a drone, and use it for exporatory motion planning </p>
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        Teaching Assistant: <a href="https://cs330.stanford.edu/">Stanford CS 330</a> [2019, 2020], Deep Multi-Task and Meta Learning <br>
        Teaching Assistant: <a href="http://www.yisongyue.com/courses/cs155/2017_winter/">Caltech CS/EE 155 [2017]</a> , Machine Learning/Data Mining <br>
        Teaching Assistant: Caltech CS 121 [2016],  Introduction to Relational Databases
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        Reviewer: NeurIPS, ICML, ICLR, CVPR, ICCV, CoRL, ICRA, IROS
      </td>
    </tr>
  </table>
</body>

  
  <a href="https://jonbarron.info/">Website Template</a>
</html>
